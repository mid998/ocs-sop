// begin header
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
:numbered:
:toc: macro
:toc-title: pass:[<b>Table of Contents</b>]
// end header
= CephOSDCriticallyFull

toc::[]

== Check Description 

*Severity:* Error

*Potential Customer Impact:* High


== Overview

A backend storage device in this Ceph Cluster has crossed 85%. Expand the storage cluster immediately.

Expand the ceph cluster by adding additional OCS worker nodes (scale out). Directions below.

== Prerequsities

=== Prerequisite 1
.Verify the alert is still active. (Required for this alert?)
----
<link> to cluster monitoring dashboard.
----
Proceed if the alert is still active. 

=== Prerequisite 3
.Setup cluster access:
----
# Setup for cluster access using normal OSD team methods. 
export KUBECONFIG=<check with OSD team - insert here>
----

=== Prerequisite 4
.Check and document ceph cluster health:
----
# Capture output using normal OSD team methods.
TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
oc rsh -n openshift-storage $TOOLS_POD

# From the rsh command prompt, run the following and capture the output.
ceph status
ceph osd status
ceph osd tree
ceph df
rados df
ceph versions
exit
----

=== Prerequisite 5
.Check the current machineset for existing wokerocs nodes:
----
# 
oc get machinesets -n openshift-machine-api | egrep 'NAME|workerocs'
----

==  Alert
=== Gather logs

.Document Ceph Cluster health check:
----
See Prerequisite 5
----

=== Make changes to solve alert

Expand the ceph cluster by creating a machineset 

.Create new MachineSets that will run storage-specific nodes for your OCP cluster: (NOTE, This may take a few minutes.)
----
CLUSTERID=$(oc get machineset -n openshift-machine-api -o jsonpath='{.items[0].metadata.labels.machine\.openshift\.io/cluster-api-cluster}')

echo $CLUSTERID

# Make this an attachment to remove external dependencies?
curl -s <link to this repo>/cluster-workerocs-us-east-2.yaml | sed -e "s/CLUSTERID/${CLUSTERID}/g" | oc apply -f -

# Be patient
----

.Verify the new machines are ready:
----
oc get machines -n openshift-machine-api | egrep 'NAME|workerocs'
----

.Scale the workerocs machineset: 
----
oc get machinesets -n openshift-machine-api -o name | grep workerocs | xargs -n1 -t oc scale -n openshift-machine-api --replicas=2
----

== Troubleshooting
* Issue encountered while following the sop 
----
Here is the solution
----
