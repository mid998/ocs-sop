<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>CephClusterCriticallyFull :: OCS Standard Operating Procedures</title>
    <link rel="canonical" href="https://red-hat-storage.github.io/ocs-sop/sop/alerts/CephClusterCriticallyFull.html">
    <meta name="description" content="Storage cluster utilization has crossed 85%.">
    <meta name="generator" content="Antora 2.3.4">
    <link rel="stylesheet" href="../../_/css/site.css">
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PDMKGVZNGE"></script>
    <script>function gtag(){dataLayer.push(arguments)};window.dataLayer=window.dataLayer||[];gtag('js',new Date());gtag('config','G-PDMKGVZNGE')</script>
    <script>var uiRootPath = '../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage" target="_blank">
          <img src="../../_/img/header_logo_reverse.svg" height="40px" alt="Red Hat Data Services">
      </a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Get Help</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_container_storage" target="_blank">OCS Documentation</a>
            <a class="navbar-item" href="https://bugzilla.redhat.com/describecomponents.cgi?product=Red%20Hat%20OpenShift%20Container%20Storage" target="_blank">Browse Bugs</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Improve Guides</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://github.com/red-hat-storage/ocs-training/blob/master/CONTRIBUTING.adoc" target="_blank">Guidelines</a>
            <a class="navbar-item" href="https://github.com/red-hat-storage/ocs-training/issues/new/choose" target="_blank">Open Issue</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">More Infos</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://www.redhat.com/en/blog/channel/red-hat-storage" target="_blank">Our Blog</a>
            <a class="navbar-item" href="https://www.youtube.com/channel/UCoyG8VyvB-XUxQl1mD3T3Gw" target="_blank">Youtube</a>
            <a class="navbar-item" href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage" target="_blank">OCS Technology</a>
          </div>
        </div>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="sop" data-version="master">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">OCS Standard Operating Procedures</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Alerts</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="basic_template.html">basic_template</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="CephClusterReadOnly.html">CephClusterReadOnly</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="CephClusterErrorState.html">CephClusterErrorState</a>
  </li>
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="CephClusterCriticallyFull.html">CephClusterCriticallyFull</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="CephOSDCriticallyFull.html">CephOSDCriticallyFull</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="CephOSDNearFull.html">CephOSDNearFull</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="CephClusterWarningState.html">CephClusterWarningState</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="CephMdsMissingReplicas.html">CephMdsMissingReplicas</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="CephMgrIsAbsent.html">CephMgrIsAbsent</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="CephMgrIsMissingReplicas.html">CephMgrIsMissingReplicas</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="CephMonQuorumAtRisk.html">CephMonQuorumAtRisk</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="CephMonVersionMismatch.html">CephMonVersionMismatch</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="CephNodeDown.html">CephNodeDown</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="CephOSDDiskNotResponding.html">CephOSDDiskNotResponding</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="CephOSDDiskUnavailable.html">CephOSDDiskUnavailable</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="CephOSDVersionMismatch.html">CephOSDVersionMismatch</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="CephPGRepairTakingTooLong.html">CephPGRepairTakingTooLong</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">OCS Standard Operating Procedures</span>
    <span class="version">master</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">OCS Standard Operating Procedures</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">master</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">OCS Standard Operating Procedures</a></li>
    <li>Alerts</li>
    <li><a href="CephClusterCriticallyFull.html">CephClusterCriticallyFull</a></li>
  </ul>
</nav>
  <div class="edit-this-page"><a href="file:///antora/sop/modules/alerts/pages/CephClusterCriticallyFull.adoc">Edit this Page</a></div>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">CephClusterCriticallyFull</h1>
<div class="sect1">
<h2 id="_title_message"><a class="anchor" href="#_title_message"></a>1. {title_message}</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Storage cluster is critically full and needs immediate expansion.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_title_description"><a class="anchor" href="#_title_description"></a>2. {title_description}</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Storage cluster utilization has crossed 85%.</p>
</div>
<div class="paragraph">
<p>Detailed Description: Storage cluster utilization has crossed 80% and will become read-only at 85%. Your Ceph cluster will become read only once utilization crosses 85%. Expand the storage cluster immediately.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_title_severity"><a class="anchor" href="#_title_severity"></a>3. {title_severity}</h2>
<div class="sectionbody">
<div class="paragraph">
<p><strong>Critical</strong></p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_prerequsities"><a class="anchor" href="#_prerequsities"></a>4. Prerequsities</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Ensure you have <a href="Prerequisites.html#verify_cluster_access">cluster admin access.</a></p>
</li>
<li>
<p>Ensure the alert is <a href="Prerequisites.html#verify_alert">still active.</a></p>
</li>
<li>
<p>Document <a href="Prerequisites.html#check_cluster_health">current cluster health.</a></p>
</li>
<li>
<p>Deploy the <a href="Prerequisites.html#deploy_toolbox">Ceph toolbox</a> if necessary.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_title_procedure"><a class="anchor" href="#_title_procedure"></a>5. {title_procedure}</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_resolution_overview"><a class="anchor" href="#_resolution_overview"></a>5.1. Resolution Overview</h3>
<div class="paragraph">
<p>Determine if you can proceed with scaling *UP* or *OUT* for expansion. Choose the appropriate tab for your deployment below.</p>
</div>
</div>
<div class="sect2">
<h3 id="_preparation_for_scaling_up"><a class="anchor" href="#_preparation_for_scaling_up"></a>5.2. Preparation for Scaling UP</h3>
<div class="paragraph">
<p>The procedure for scaling <strong>UP</strong> storage requires adding more storage capacity to existing nodes. In general, this process requires 3 steps:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Check Ceph Cluster Status Before Recovery</strong> - Check ceph status, ceph osd status, check current alerts</p>
</li>
<li>
<p><strong>Add Storage Capacity</strong> - Determine if LSO is in use or not, add capacity accordingly. This will be deployment specific and you will need to have some idea of how your OCS cluster was deployed (i.e. AWS, Azure, bare metal) and how to add storage for your specific deployment.</p>
</li>
<li>
<p><strong>Check Ceph Cluster Status During Recovery</strong> - Essentially same as first item.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_assess_your_cluster_state"><a class="anchor" href="#_assess_your_cluster_state"></a>5.3. Assess Your Cluster State</h3>
<div class="paragraph">
<p>Check the current status of the ceph cluster and OSDs using the rook-ceph toolbox:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code>oc patch OCSInitialization ocsinit -n openshift-storage --type json --patch  '[{ "op": "replace", "path": "/spec/enableCephTools", "value": true }]'</code></pre>
</div>
</div>
<div class="paragraph">
<p>After the <code>rook-ceph-tools</code> <strong>Pod</strong> is <code>Running</code>, access the <strong>toolbox</strong> like this:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code>TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
oc rsh -n openshift-storage $TOOLS_POD</code></pre>
</div>
</div>
<div class="paragraph">
<p>Once inside the <strong>toolbox</strong>, run the following Ceph commands:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code>ceph status</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the output of <code>ceph status</code> look for cluster: health, and data: usage.</p>
</div>
<div class="paragraph">
<p><strong>Typically</strong>, OSDs running out of space to replicate are causing the problem. View current OSD status by running:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code>ceph osd status</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the output of <code>ceph osd status</code> look for the numbers in the used and avail columns. You will track these numbers after adding scaling storage capacity.</p>
</div>
<div class="paragraph">
<p>Do not forget to exit the pod to return back to your command prompt:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code>exit</code></pre>
</div>
</div>
<div class="paragraph">
<p>This in this situation, it is common to have <strong>MANY</strong> alerts firing or warning all at once. You may track current alert statuses by running:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>WIP: findall_alerts.sh instructions here</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_determine_lso_or_no_lso"><a class="anchor" href="#_determine_lso_or_no_lso"></a>5.4. Determine LSO or No LSO</h3>
<div class="paragraph">
<p>To determine is LSO is installed and in use, run the following commands:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code>oc get csv -A | grep local-storage</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>openshift-local-storage                local-storage-operator.4.6.0-202103130248.p0   Local Storage                 4.6.0-202103130248.p0              Succeeded</pre>
</div>
</div>
<div class="paragraph">
<p>If you see output like above ending with Succeeded, LSO is installed.</p>
</div>
<div class="paragraph">
<p>Determine if LSO is in use by looking for PVs that begin with the "local-pv" prefix, and PVC names that contain "ocs-deviceset-" prefix then the name of the storageClass used in the storage cluster definition, by running the following commands:</p>
</div>
<div class="listingblock execute">
<div class="title">Look for PV names with local-pv prefixes:</div>
<div class="content">
<pre class="highlightjs highlight"><code>oc -n openshift-storage get pvc | grep local-pv</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>ocs-deviceset-localblksc-demo-0-data-0-jlp6g   Bound    local-pv-f4d1075c                          50Gi       RWO            localblkscdemo               140m
ocs-deviceset-localblksc-demo-1-data-0-c8pmq   Bound    local-pv-bf5ca51e                          50Gi       RWO            localblkscdemo               140m
ocs-deviceset-localblksc-demo-2-data-0-nzzn5   Bound    local-pv-fb64d972                          50Gi       RWO            localblkscdemo               140m</pre>
</div>
</div>
<div class="paragraph">
<p>Notice the storage class name embedded in the name of the PVC, right after "ocs-deviceset-". In our example demo case, this is "localblksc-demo" and can be verified to be in use by the storage cluster by running:</p>
</div>
<div class="listingblock execute">
<div class="title">Check the storageClassName in the storage cluster matches:</div>
<div class="content">
<pre class="highlightjs highlight"><code>oc -n openshift-storage get storagecluster -o yaml | grep storageClassName</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>storageClassName: localblksc-demo</pre>
</div>
</div>
<div class="paragraph">
<p>If PVs named local-pv* are present and are bound to PVCs that contain the name of the storageClass in use by storagecluster definition, LSO is in use.</p>
</div>
</div>
<div class="sect2">
<h3 id="_adding_storage_capacity_when_local_storage_operator_is_in_use"><a class="anchor" href="#_adding_storage_capacity_when_local_storage_operator_is_in_use"></a>5.5. Adding Storage Capacity when Local Storage Operator is in USE</h3>
<div class="paragraph">
<p>To determine which nodes will need additional capacity, view the localvolumeset and look for values: with key: kubernetes.io/hostname by running the following command:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code>oc -n openshift-local-storage get localvolumeset -o yaml</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>[...]
    nodeSelector:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - ip-10-0-149-49
          - ip-10-0-179-37
          - ip-10-0-209-228
    storageClassName: localblksc-demo
[...]</pre>
</div>
</div>
<div class="paragraph">
<p>Add storage capacity (i.e. in AWS attach EBS volumes) to each hostname listed. Details of attaching additional storage capacity will depend on your particular environment and are beyond the scope of this document.</p>
</div>
<div class="paragraph">
<p>Once you have added capacity to each node, verify LSO has discovered the new storage by viewing the localvolumediscoveryresults details. To view the localvolumediscoveryresults details, first view the localvolumediscovery then choose a node for a spot check:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code>oc -n openshift-local-storage get localvolumediscoveryresults</code></pre>
</div>
</div>
<div class="paragraph">
<p>If you do not see your new storage please rerun the command a few times to allow discovery to complete.</p>
</div>
<div class="paragraph">
<p>Choose one of the localvolumediscoveryresults from above to inspect details. When new storage has been "discovered" it will be reflected under status: discoveredDevices.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code>oc -n openshift-local-storage get localvolumediscoveryresults/discovery-result-ip-10-0-149-49.us-east-2.compute.internal -o yaml</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>[...]
  - deviceID: /dev/disk/by-id/nvme-Amazon_Elastic_Block_Store_vol0b86476e1d962d061
    fstype: ""
    model: 'Amazon Elastic Block Store              '
    path: /dev/nvme2n1
    property: NonRotational
    serial: vol0b86476e1d962d061
    size: 53687091200
    status:
      state: Available
    type: disk
    vendor: ""
[...]</pre>
</div>
</div>
<div class="paragraph">
<p>Expand the OCS cluster by directly editing the ocs-storagecluster definition, finding spec: count: and increasing the count by 1:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code>oc -n openshift-storage edit storagecluster/ocs-storagecluster</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>spec:
  encryption: {}
  externalStorage: {}
  [...]
  monDataDirHostPath: /var/lib/rook
  storageDeviceSets:
  - config: {}
    count: 1</pre>
</div>
</div>
<div class="paragraph">
<p>In the example above "count: 1" becomes "count: 2".</p>
</div>
<div class="paragraph">
<p>A successful edit of storagecluster/ocs-storagecluster will show the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>storagecluster.ocs.openshift.io/ocs-storagecluster edited</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_assess_your_cluster_during_recovery"><a class="anchor" href="#_assess_your_cluster_during_recovery"></a>5.6. Assess Your Cluster During Recovery</h3>
<div class="paragraph">
<p>Watch the expansion progress by viewing ceph status and ceph osd status. This step is a repeat of Assess Your Cluster however, this time you will be specifically looking for evidence of:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>New osds for the new storage</p>
</li>
<li>
<p>Status of recovery in io:</p>
</li>
<li>
<p>Progress of OSD rebalancing across all (existing and new) OSDs</p>
</li>
<li>
<p>Finally ceph health: and all alerts return to normal</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In case you did not do this from the previous Assess Your Cluster section:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code>oc patch OCSInitialization ocsinit -n openshift-storage --type json --patch  '[{ "op": "replace", "path": "/spec/enableCephTools", "value": true }]'</code></pre>
</div>
</div>
<div class="paragraph">
<p>After the <code>rook-ceph-tools</code> <strong>Pod</strong> is <code>Running</code>, access the <strong>toolbox</strong> like this:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code>TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
oc rsh -n openshift-storage $TOOLS_POD</code></pre>
</div>
</div>
<div class="paragraph">
<p>Once inside the <strong>toolbox</strong>, run the following Ceph commands:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code>ceph status</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output:</div>
<div class="content">
<pre>  cluster:
    id:     4e949bf7-7e24-4c0e-898e-f5985b514a99
    health: HEALTH_ERR
            3 backfillfull osd(s)
            3 pool(s) backfillfull
            Degraded data redundancy: 18634/33627 objects degraded (55.414%), 122 pgs degraded
            Full OSDs blocking recovery: 93 pgs recovery_toofull

  services:
    mon: 3 daemons, quorum a,b,c (age 59m)
    mgr: a(active, since 59m)
    mds: ocs-storagecluster-cephfilesystem:1 {0=ocs-storagecluster-cephfilesystem-a=up:active} 1 up:standby-replay
    osd: 6 osds: 6 up (since 43s), 6 in (since 43s)

  task status:
    scrub status:
        mds.ocs-storagecluster-cephfilesystem-a: idle
        mds.ocs-storagecluster-cephfilesystem-b: idle

  data:
    pools:   3 pools, 288 pgs
    objects: 11.21k objects, 43 GiB
    usage:   137 GiB used, 163 GiB / 300 GiB avail
    pgs:     18634/33627 objects degraded (55.414%)
             45/33627 objects misplaced (0.134%)
             165 active+clean
             93  active+recovery_toofull+degraded
             29  active+recovery_wait+degraded
             1   active+recovering

  io:
    client:   3.0 KiB/s rd, 75 MiB/s wr, 3 op/s rd, 38 op/s wr
    recovery: 71 MiB/s, 0 keys/s, 20 objects/s</pre>
</div>
</div>
<div class="paragraph">
<p>Items to notice:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The cluster: health: is still HEALTH_ERR which is expected until the cluster full recovers.</p>
</li>
<li>
<p>service: osds reflect the total number of desired OSDs, and how many are currently up. This should eventually change to have desired match up.</p>
</li>
<li>
<p>io: recovery: being present means a recovery is taking place. This line will disappear when the recovery is complete.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To actively watch the OSD recovery, run the following:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code>ceph osd status</code></pre>
</div>
</div>
<div class="paragraph">
<p>Watch the output of <code>ceph osd status</code> for detail on how the OSDs are rebalancing. You will see changes in the used and avail columns as ceph moves data to achieve a health state.</p>
</div>
<div class="listingblock">
<div class="title">Example output while data is being rebalanced:</div>
<div class="content">
<pre>+----+--------------------------------------------+-------+-------+--------+---------+--------+---------+-----------+
| id |                    host                    |  used | avail | wr ops | wr data | rd ops | rd data |   state   |
+----+--------------------------------------------+-------+-------+--------+---------+--------+---------+-----------+
| 0  | ip-10-0-179-37.us-east-2.compute.internal  | 33.9G | 16.0G |    0   |   830k  |    0   |     0   | exists,up |
| 1  | ip-10-0-209-228.us-east-2.compute.internal | 35.0G | 14.9G |    3   |  5652k  |    0   |     0   | exists,up |
| 2  | ip-10-0-149-49.us-east-2.compute.internal  | 32.4G | 17.5G |    0   |  1638k  |    0   |     0   | exists,up |
| 3  | ip-10-0-179-37.us-east-2.compute.internal  | 13.5G | 86.4G |    3   |  8532k  |    2   |   106   | exists,up |
| 4  | ip-10-0-209-228.us-east-2.compute.internal | 12.2G | 87.7G |    8   |  8183k  |    0   |     0   | exists,up |
| 5  | ip-10-0-149-49.us-east-2.compute.internal  | 15.2G | 84.7G |    3   |  4118k  |    0   |     0   | exists,up |
+----+--------------------------------------------+-------+-------+--------+---------+--------+---------+-----------+</pre>
</div>
</div>
<div class="paragraph">
<p>Once the recovery process has completed, <code>ceph status</code> will show:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>cluster: health: HEALTH_OK</p>
</li>
<li>
<p>All OSDs desired and present in service: osds:</p>
</li>
<li>
<p>io: recovery: will not be present since recovery has completed</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In addition, <code>ceph osd status</code> will show:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Fairly even distribution of used/avail across all OSDs</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="title">Example out of <code>ceph osd status</code> after recovery:</div>
<div class="content">
<pre>+----+--------------------------------------------+-------+-------+--------+---------+--------+---------+-----------+
| id |                    host                    |  used | avail | wr ops | wr data | rd ops | rd data |   state   |
+----+--------------------------------------------+-------+-------+--------+---------+--------+---------+-----------+
| 0  | ip-10-0-209-93.us-east-2.compute.internal  | 1104M | 48.9G |    0   |     0   |    0   |     0   | exists,up |
| 1  | ip-10-0-140-27.us-east-2.compute.internal  | 1116M | 48.9G |    0   |     0   |    0   |     0   | exists,up |
| 2  | ip-10-0-183-136.us-east-2.compute.internal | 1092M | 48.9G |    0   |  3276   |    0   |     0   | exists,up |
| 3  | ip-10-0-140-27.us-east-2.compute.internal  | 1110M | 48.9G |    0   |  7372   |    0   |     0   | exists,up |
| 4  | ip-10-0-183-136.us-east-2.compute.internal | 1134M | 48.8G |    0   |  2457   |    0   |     0   | exists,up |
| 5  | ip-10-0-209-93.us-east-2.compute.internal  | 1121M | 48.9G |    0   |     0   |    2   |   106   | exists,up |
+----+--------------------------------------------+-------+-------+--------+---------+--------+---------+-----------+</pre>
</div>
</div>
<div class="paragraph">
<p>Do not forget to exit the pod to return back to your command prompt:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code>exit</code></pre>
</div>
</div>
<div class="paragraph">
<p>Alerts will resolve themselves as the cluster recovers.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>WIP: findall_alerts.sh instructions here</pre>
</div>
</div>
<div class="paragraph">
<p>This procedure is complete.
=== Adding Storage Capacity when Local Storage Operator is NOT in USE</p>
</div>
<div class="paragraph">
<p>The procedure for scaling up capacity on AWS, Azure or VMWare is the same. Please follow the steps below.</p>
</div>
<div class="listingblock">
<div class="title">Check the current number of OSD pods and their status:</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pod -o=custom-columns=NAME:.metadata.name,STATUS:.status.phase,NODE:.spec.nodeName -n openshift-storage | grep osd | grep -v prepare</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>rook-ceph-osd-0-5686cb6949-q6t4r                                  Running     perf7-m4ngt-worker-vswpc
rook-ceph-osd-1-5ff8ffd585-w72vm                                  Running     perf7-m4ngt-worker-vwkqk
rook-ceph-osd-2-745d64bcff-s6kkn                                  Running     perf7-m4ngt-worker-7vq2w</pre>
</div>
</div>
<div class="listingblock">
<div class="title">Increase the storagecluster spec:count by editing the storagecluster directly:</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc edit storagecluster/ocs-storagecluster</code></pre>
</div>
</div>
<div class="paragraph">
<p>Search for count: and increase by 1. Save and exit your editor. Watch for the new OSD pods to be in a running state.</p>
</div>
<div class="listingblock">
<div class="title">Example</div>
<div class="content">
<pre>[...]
spec:
  encryption: {}
  externalStorage: {}
  managedResources:
    cephBlockPools: {}
    cephFilesystems: {}
    cephObjectStoreUsers: {}
    cephObjectStores: {}
  storageDeviceSets:
  - config: {}
    count: 1     &lt;&lt;&lt;&lt;&lt; increase by 1
[...]</pre>
</div>
</div>
<div class="paragraph">
<p>Caution
It may take more than 5 minutes for new OSD pods to be in a Running state.
.Step 3: Check the current number of OSD pods and their status after increasing the storagecluster count:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc get pod -o=custom-columns=NAME:.metadata.name,STATUS:.status.phase,NODE:.spec.nodeName -n openshift-storage | grep osd | grep -v prepare</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Example output</div>
<div class="content">
<pre>rook-ceph-osd-0-5686cb6949-q6t4r                                  Running     perf7-m4ngt-worker-vswpc
rook-ceph-osd-1-5ff8ffd585-w72vm                                  Running     perf7-m4ngt-worker-vwkqk
rook-ceph-osd-2-745d64bcff-s6kkn                                  Running     perf7-m4ngt-worker-7vq2w
rook-ceph-osd-3-86dbcddc96-dmv9s                                  Pending     &lt;none&gt;
rook-ceph-osd-4-7d6f55bc6d-sctdc                                  Pending     &lt;none&gt;
rook-ceph-osd-5-67dc5d75f9-gthp2                                  Pending     &lt;none&gt;</pre>
</div>
</div>
<div class="listingblock">
<div class="title">Step 4: Verify the added the capacity by checking the cluster status:</div>
<div class="content">
<pre>step 5</pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_title_logs"><a class="anchor" href="#_title_logs"></a>6. {title_logs}</h2>
<div class="sectionbody">
<div class="listingblock">
<div class="title">Document Ceph Cluster health check:</div>
<div class="content">
<pre>For OCS specific results run:
oc adm must-gather --image=registry.redhat.io/ocs4/ocs-must-gather-rhel8:v4.6</pre>
</div>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <a class="navbar-item" href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage" target="_blank">
      <img src="../../_/img/header_logo.svg" alt="Red Hat Data Services">
  </a>
</footer>
<script src="../../_/js/site.js"></script>
<script async src="../../_/js/vendor/highlight.js"></script>
  </body>
</html>
